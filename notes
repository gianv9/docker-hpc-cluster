ideas 26/02/19:

Now that we've found one way to set the shared filesystem on the nodes 
(see my dockerhub starred repos).
We must take into consideration some other factors like speed, the architecture the found image uses,
how hard or how well is it going to be to integrate the image into the project, so the nodes can mount it.
For example, there are other plugins and connectors that enable storage accross the containers within
a swarm cluster.

One main advantage of using the nfs share image found at dockerhub is that is can be deployed
as a service, which follows the microservice architecture docker promotes and supports.

also, we must define a standarized way to launch the clusters.
The best way i can think of is to define a compose file that launches the alpine cluster.
but that is going to bring some challenges:
- how to parametrize the names, ports, repos that are used to create/launch the image? 
- how is the solution going to scale up or down properly by using docker stacks?

next steps,

now that we now that the shared filesystem problem is already solved, 
we must standarize the way the projects are launched. After that, the previously mentioned
filesystem share will be done.


01/03/18:

It is important that lanching stript uses the user's account (just like on the alpine projetc),
So that anyone is able to push and pull the images to and from their own account's repo.
Otherwise they aren't going to be able to launch the cluster.

Note: we must remember to specify the thought process we went through when starting this project.
For example, we had the idea (automatically spawn a cluster by using docker containers), but as 
we searched for similar solutions we came accross two projects (nguyen's and filgueira's) on which
we focused on. So instead of doing the same thing they did again (reinventing the wheel), we decided
to build on top of those projects and improve them even more, integrating them into a single, very
simple to use, and multi envirornment hpc cluster deployment project.

Then we also found an eyeopening article about hpc and mpi (here we put the reference to the canadian
guy's article), that dives into the history and future of mpi as a thechlonogy on the hpc world.
which in turn lead us to further research into the currently used hpc programming solution such as
earlang and julia.